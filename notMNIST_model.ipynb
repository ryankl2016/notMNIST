{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "This dataset was created by Yaroslav Bulatov by taking some publicly available fonts and extracting glyphs from them to make a dataset similar to MNIST. There are 10 classes, with letters A-J.\n",
    "\n",
    "## Content\n",
    "\n",
    "A set of training and test images of letters from A to J on various typefaces. The images size is 28x28 pixels.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "The dataset can be found on Tensorflow github page as well as on the blog from Yaroslav, here.\n",
    "\n",
    "## Inspiration\n",
    "\n",
    "This is a pretty good dataset to train classifiers! According to Yaroslav:\n",
    "\n",
    "Judging by the examples, one would expect this to be a harder task than MNIST. This seems to be the case -- logistic regression on top of stacked auto-encoder with fine-tuning gets about 89% accuracy whereas same approach gives got 98% on MNIST. Dataset consists of small hand-cleaned part, about 19k instances, and large uncleaned dataset, 500k instances. Two parts have approximately 0.5% and 6.5% label error rate. I got this by looking through glyphs and counting how often my guess of the letter didn't match it's unicode value in the font file.\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanleung/anaconda3/envs/py3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the path names to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small/\n",
      "0 0\n",
      "1000 1000\n",
      "2000 2000\n",
      "3000 3000\n",
      "4000 3999\n",
      "5000 4999\n",
      "6000 5999\n",
      "7000 6998\n",
      "8000 7998\n",
      "9000 8998\n",
      "10000 9998\n",
      "11000 10998\n",
      "12000 11998\n",
      "13000 12998\n",
      "14000 13998\n",
      "15000 14998\n",
      "16000 15998\n",
      "17000 16998\n",
      "18000 17998\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>notMNIST_small/I/Qml0d2lzZS50dGY=.png</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>notMNIST_small/I/RW5nbGFuZCBCb2xkSXRhbGljLnR0Z...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>notMNIST_small/I/R3JlZWsgSXRhbGljLnR0Zg==.png</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>notMNIST_small/I/Rmx5d2hlZWxTcXVhcmUudHRm.png</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>notMNIST_small/I/SGFuZGljYXAub3Rm.png</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0              notMNIST_small/I/Qml0d2lzZS50dGY=.png  I\n",
       "1  notMNIST_small/I/RW5nbGFuZCBCb2xkSXRhbGljLnR0Z...  I\n",
       "2      notMNIST_small/I/R3JlZWsgSXRhbGljLnR0Zg==.png  I\n",
       "3      notMNIST_small/I/Rmx5d2hlZWxTcXVhcmUudHRm.png  I\n",
       "4              notMNIST_small/I/SGFuZGljYXAub3Rm.png  I"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parentDir = 'notMNIST_small/'\n",
    "print(parentDir)\n",
    "data = []\n",
    "total = 0\n",
    "good = 0\n",
    "for folder in os.listdir(parentDir):\n",
    "    if folder != '.DS_Store':\n",
    "        for file in os.listdir(parentDir + folder):\n",
    "            if total % 1000 == 0:\n",
    "                print(total, good)\n",
    "            total += 1\n",
    "            try:\n",
    "                img_path = parentDir + folder + '/' + file\n",
    "                img = Image.open(img_path)\n",
    "                data.append([img_path, folder])\n",
    "                good += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "dataset = pd.DataFrame(data)\n",
    "dataset.head()\n",
    "\n",
    "\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18724\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set batch size and epochs\n",
    "\n",
    "Don't want batch size to be too large or not too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "def input_func(features, labels, batch_size, mode):\n",
    "    \n",
    "    def parser(image, label): \n",
    "        \n",
    "        img = tf.image.decode_png(tf.read_file(image))\n",
    "        img = tf.reshape(img, [28, 28, 1])\n",
    "        img = tf.cast(img, tf.float32, \"cast\")\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    dataset = dataset.map(parser)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN: \n",
    "        dataset = dataset.repeat(12)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model architecture\n",
    "Uses a two layer, each layer consisting of a convolutional and pooling layer, architecture. (Same architecture as original MNIST CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(features, labels, mode, params):\n",
    "    #initialize input by reshaping and casting for network\n",
    "    #img = tf.image.decode_png(tf.read_file(features['x'][0]))\n",
    "    # img = np.array( img, dtype='uint8' ).flatten()\n",
    "    \n",
    "    # FIRST LAYER\n",
    "    # ---conv layer with 32 filters, 5x5 kernel, and relu activation\n",
    "    # ---pool layer with 2x2 pool window and stride of 2x2\n",
    "    conv1 = tf.layers.conv2d(inputs=features, filters=64, kernel_size=(5, 5), padding=\"same\", activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(2, 2), strides=(2, 2))\n",
    "    \n",
    "    # SECOND LAYER\n",
    "    # ---conv layer with 64 filters, 5x5 kernel, and relu activation\n",
    "    # ---pool layer with 2x2 pool window and stride of 2x2\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1, filters=128, kernel_size=(5, 5), padding=\"same\", activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2, 2), strides=(2, 2))\n",
    "    \n",
    "#     print(pool2.shape)\n",
    "    \n",
    "    # DENSE LAYER\n",
    "    # ---flatten output into vector\n",
    "    # ---dropout to prevent overfitting\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 128])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.2, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "    onehot_labels = tf.reshape(onehot_labels, [-1, 10])\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
    "#     loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    print(labels.shape)\n",
    "    print(predictions[\"classes\"].shape)\n",
    "    eval_metric_ops = {\"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating\n",
    "-> split data into train and test (2:1)\n",
    "-> instantiate model with my_mode as cnn\n",
    "-> convert dataset (np array) to dataframe to use pd.factorize to get integer labels, then convert back to np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/th/svpqqvhs62790bm9gczzcth40000gn/T/tmpgzpz4h9z\n",
      "INFO:tensorflow:Using config: {'_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_global_id_in_cluster': 0, '_task_type': 'worker', '_num_worker_replicas': 1, '_save_summary_steps': 100, '_master': '', '_task_id': 0, '_keep_checkpoint_max': 5, '_num_ps_replicas': 0, '_log_step_count_steps': 100, '_service': None, '_save_checkpoints_secs': 600, '_train_distribute': None, '_save_checkpoints_steps': None, '_session_config': None, '_is_chief': True, '_model_dir': '/var/folders/th/svpqqvhs62790bm9gczzcth40000gn/T/tmpgzpz4h9z', '_evaluation_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a27491c50>}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function my_model at 0x1a23a88c80>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "(?, 7, 7, 128)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/th/svpqqvhs62790bm9gczzcth40000gn/T/tmpgzpz4h9z/model.ckpt.\n",
      "INFO:tensorflow:loss = 55.299988, step = 1\n",
      "INFO:tensorflow:global_step/sec: 9.39816\n",
      "INFO:tensorflow:loss = 0.81384325, step = 101 (10.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.48354\n",
      "INFO:tensorflow:loss = 0.7431114, step = 201 (10.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0619\n",
      "INFO:tensorflow:loss = 0.15230599, step = 301 (9.938 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.61384\n",
      "INFO:tensorflow:loss = 0.3329313, step = 401 (10.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.65898\n",
      "INFO:tensorflow:loss = 0.90354264, step = 501 (10.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.90356\n",
      "INFO:tensorflow:loss = 0.7213824, step = 601 (10.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.6157\n",
      "INFO:tensorflow:loss = 0.23496151, step = 701 (10.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.11679\n",
      "INFO:tensorflow:loss = 0.88115394, step = 801 (12.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.82578\n",
      "INFO:tensorflow:loss = 0.3081541, step = 901 (10.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.06031\n",
      "INFO:tensorflow:loss = 0.5655901, step = 1001 (11.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.95498\n",
      "INFO:tensorflow:loss = 0.11799217, step = 1101 (10.044 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5054\n",
      "INFO:tensorflow:loss = 0.065078825, step = 1201 (9.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3704\n",
      "INFO:tensorflow:loss = 0.5954511, step = 1301 (9.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4806\n",
      "INFO:tensorflow:loss = 0.14713073, step = 1401 (9.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.71795\n",
      "INFO:tensorflow:loss = 0.13536751, step = 1501 (10.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.20983\n",
      "INFO:tensorflow:loss = 0.2355198, step = 1601 (10.859 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.10464\n",
      "INFO:tensorflow:loss = 0.5406379, step = 1701 (12.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.19413\n",
      "INFO:tensorflow:loss = 0.27109367, step = 1801 (10.877 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.30905\n",
      "INFO:tensorflow:loss = 0.42097425, step = 1901 (12.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.18403\n",
      "INFO:tensorflow:loss = 0.024658917, step = 2001 (12.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.05252\n",
      "INFO:tensorflow:loss = 0.48352277, step = 2101 (12.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.37328\n",
      "INFO:tensorflow:loss = 0.10236699, step = 2201 (10.669 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.55569\n",
      "INFO:tensorflow:loss = 0.14701551, step = 2301 (11.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.06307\n",
      "INFO:tensorflow:loss = 0.14123002, step = 2401 (12.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.4495\n",
      "INFO:tensorflow:loss = 0.011681718, step = 2501 (10.582 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.98879\n",
      "INFO:tensorflow:loss = 0.03711072, step = 2601 (12.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.90718\n",
      "INFO:tensorflow:loss = 0.063515216, step = 2701 (12.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.66811\n",
      "INFO:tensorflow:loss = 0.07255048, step = 2801 (10.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.57466\n",
      "INFO:tensorflow:loss = 0.08413207, step = 2901 (11.663 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.92281\n",
      "INFO:tensorflow:loss = 0.022909986, step = 3001 (10.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2241\n",
      "INFO:tensorflow:loss = 0.41712758, step = 3101 (9.781 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.76332\n",
      "INFO:tensorflow:loss = 0.17459278, step = 3201 (10.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1756\n",
      "INFO:tensorflow:loss = 0.3920048, step = 3301 (9.826 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.87921\n",
      "INFO:tensorflow:loss = 0.19178699, step = 3401 (10.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.05598\n",
      "INFO:tensorflow:loss = 0.08202603, step = 3501 (12.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3743\n",
      "INFO:tensorflow:loss = 0.37725082, step = 3601 (9.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.51567\n",
      "INFO:tensorflow:loss = 0.0060631693, step = 3701 (10.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.03842\n",
      "INFO:tensorflow:loss = 0.1637972, step = 3801 (11.063 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.18354\n",
      "INFO:tensorflow:loss = 0.06738424, step = 3901 (10.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.942\n",
      "INFO:tensorflow:loss = 0.19542226, step = 4001 (11.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.20212\n",
      "INFO:tensorflow:loss = 0.18204784, step = 4101 (10.866 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3717\n",
      "INFO:tensorflow:loss = 0.4802668, step = 4201 (9.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1805\n",
      "INFO:tensorflow:loss = 0.15278555, step = 4301 (9.822 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0476\n",
      "INFO:tensorflow:loss = 0.112098135, step = 4401 (9.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1177\n",
      "INFO:tensorflow:loss = 0.016358618, step = 4501 (9.884 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.93846\n",
      "INFO:tensorflow:loss = 0.11518878, step = 4601 (10.064 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4807\n",
      "INFO:tensorflow:loss = 0.005630423, step = 4701 (9.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0547\n",
      "INFO:tensorflow:loss = 0.118871935, step = 4801 (9.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3613\n",
      "INFO:tensorflow:loss = 0.015849216, step = 4901 (9.651 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1497\n",
      "INFO:tensorflow:loss = 0.040921673, step = 5001 (9.852 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.63932\n",
      "INFO:tensorflow:loss = 0.06303887, step = 5101 (10.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.30774\n",
      "INFO:tensorflow:loss = 0.08259179, step = 5201 (12.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.49952\n",
      "INFO:tensorflow:loss = 0.059422366, step = 5301 (10.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1625\n",
      "INFO:tensorflow:loss = 0.013758638, step = 5401 (9.840 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.70523\n",
      "INFO:tensorflow:loss = 0.13558903, step = 5501 (10.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.46537\n",
      "INFO:tensorflow:loss = 0.20034246, step = 5601 (10.565 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5630 into /var/folders/th/svpqqvhs62790bm9gczzcth40000gn/T/tmpgzpz4h9z/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 8.89943\n",
      "INFO:tensorflow:loss = 0.14346527, step = 5701 (11.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.17584\n",
      "INFO:tensorflow:loss = 0.2006302, step = 5801 (10.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1424\n",
      "INFO:tensorflow:loss = 0.011336012, step = 5901 (9.858 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.86003\n",
      "INFO:tensorflow:loss = 0.041090414, step = 6001 (10.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.9663\n",
      "INFO:tensorflow:loss = 0.013718298, step = 6101 (10.034 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.55796\n",
      "INFO:tensorflow:loss = 0.00571769, step = 6201 (10.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.95331\n",
      "INFO:tensorflow:loss = 0.00436904, step = 6301 (10.047 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.83469\n",
      "INFO:tensorflow:loss = 0.044809755, step = 6401 (10.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1619\n",
      "INFO:tensorflow:loss = 0.049542263, step = 6501 (9.841 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.47974\n",
      "INFO:tensorflow:loss = 0.49414176, step = 6601 (10.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.51481\n",
      "INFO:tensorflow:loss = 0.082127854, step = 6701 (10.510 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 9.85202\n",
      "INFO:tensorflow:loss = 0.016113032, step = 6801 (10.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1423\n",
      "INFO:tensorflow:loss = 0.048931718, step = 6901 (9.860 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.8569\n",
      "INFO:tensorflow:loss = 0.0025207207, step = 7001 (10.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.9018\n",
      "INFO:tensorflow:loss = 0.0018015531, step = 7101 (10.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2388\n",
      "INFO:tensorflow:loss = 0.0020498815, step = 7201 (9.767 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.275\n",
      "INFO:tensorflow:loss = 0.009991124, step = 7301 (9.732 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.50265\n",
      "INFO:tensorflow:loss = 0.03612329, step = 7401 (10.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.46723\n",
      "INFO:tensorflow:loss = 0.0052235397, step = 7501 (11.811 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.51629\n",
      "INFO:tensorflow:loss = 0.0025830683, step = 7601 (10.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3355\n",
      "INFO:tensorflow:loss = 0.15688726, step = 7701 (9.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.85175\n",
      "INFO:tensorflow:loss = 0.13869603, step = 7801 (10.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.47894\n",
      "INFO:tensorflow:loss = 0.00023919927, step = 7901 (10.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.68534\n",
      "INFO:tensorflow:loss = 0.035007745, step = 8001 (10.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.58792\n",
      "INFO:tensorflow:loss = 0.13096206, step = 8101 (10.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.47537\n",
      "INFO:tensorflow:loss = 0.01566228, step = 8201 (10.556 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.15917\n",
      "INFO:tensorflow:loss = 0.06492603, step = 8301 (10.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.12774\n",
      "INFO:tensorflow:loss = 0.008322364, step = 8401 (10.955 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1492\n",
      "INFO:tensorflow:loss = 0.056548826, step = 8501 (9.853 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.78934\n",
      "INFO:tensorflow:loss = 0.16662925, step = 8601 (10.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.54454\n",
      "INFO:tensorflow:loss = 0.08353722, step = 8701 (10.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.73444\n",
      "INFO:tensorflow:loss = 0.011509267, step = 8801 (10.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0327\n",
      "INFO:tensorflow:loss = 0.027023237, step = 8901 (9.967 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.64426\n",
      "INFO:tensorflow:loss = 0.0034371126, step = 9001 (10.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.81793\n",
      "INFO:tensorflow:loss = 0.048362218, step = 9101 (10.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.23223\n",
      "INFO:tensorflow:loss = 0.0056839753, step = 9201 (12.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.93064\n",
      "INFO:tensorflow:loss = 0.006234621, step = 9301 (11.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.39026\n",
      "INFO:tensorflow:loss = 0.0032450836, step = 9401 (10.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.24301\n",
      "INFO:tensorflow:loss = 0.0043640276, step = 9501 (10.817 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.44433\n",
      "INFO:tensorflow:loss = 0.035941217, step = 9601 (10.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2876\n",
      "INFO:tensorflow:loss = 0.0015177835, step = 9701 (9.721 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.86394\n",
      "INFO:tensorflow:loss = 0.0131342085, step = 9801 (10.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.60728\n",
      "INFO:tensorflow:loss = 0.022451805, step = 9901 (10.409 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /var/folders/th/svpqqvhs62790bm9gczzcth40000gn/T/tmpgzpz4h9z/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0016915009.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "(?, 7, 7, 128)\n",
      "(?,)\n",
      "(?,)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-29-00:56:29\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/th/svpqqvhs62790bm9gczzcth40000gn/T/tmpgzpz4h9z/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-29-00:58:00\n",
      "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.94205606, global_step = 10000, loss = 0.26205367\n",
      "\n",
      "Test set accuracy: 0.942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[0], pd.factorize(dataset[1])[0], test_size=0.20, random_state=42)\n",
    "\n",
    "# Build CNN.\n",
    "classifier = tf.estimator.Estimator(model_fn=my_model)\n",
    "\n",
    "# Train the Model.\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "classifier.train(input_fn=lambda:input_func(X_train, y_train, batch_size, tf.estimator.ModeKeys.TRAIN), steps = 10000)\n",
    "\n",
    "# Evaluate the model.\n",
    "\n",
    "eval_result = classifier.evaluate(input_fn=lambda:input_func(X_test, y_test, batch_size, tf.estimator.ModeKeys.EVAL))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sample = {\n",
    "    'a': [2, 89, 3671],\n",
    "    'b': [23, 37, 39],\n",
    "    'c': [81],\n",
    "    'd': [47, 3647],\n",
    "    'e': [95, 3648],\n",
    "    'f': [3709], \n",
    "    'g': [3718, 3704],\n",
    "    'h': [22, 35, 40],\n",
    "    'i': [3715, 3691],\n",
    "    'j': [3725, 3732]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'I', 1: 'G', 2: 'A', 3: 'F', 4: 'H', 5: 'J', 6: 'C', 7: 'D', 8: 'E', 9: 'B'}\n"
     ]
    }
   ],
   "source": [
    "class_dict = {}\n",
    "i = 0\n",
    "while len(class_dict.keys()) < 10:\n",
    "    class_dict[y_test[i]] = X_test[i].split('/')[1]\n",
    "    i += 1\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABSUlEQVR4nG2SvUpcURSF1z3njo4kjQEJiI6QdIngQ4TYiIhWsbayHesEhSQkVUjETss8QJrgD1gpCSl8Bq1SOCCIDDj37P2lGI3cObPaj7XX2vsc6b9G9IHK6SvhHD0wBbXe/rI7yPXh1rRqCtrAHBLfJ1XUWdHUG8ypOBhVLOu0kPaocLoLGhkwKmj2FsfYV6lc2xhgrwYDpULPuoBzHHJbqV0cEq+V0aCXfzEqfjTzqaW+UeHOcl4nau4SI3GkMKTORyqc3opizmZuAOckD1ShLzgY88N2bHVwjMOxHDb0FYPEYp4Y9eICo+JnIzdGvaeHk1bzHYOeX+IYp/nhJG2SAJaGwadXgPM7KyOV+tx/juXcGDV1jpE4mxisWsagXRL0eKdmDQYle7KzRpSkR+pRMz5uH3S4/+d/1sdrgz9h/S0AjLYaD+wfWm3b41U2imcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"I\" (99.989%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABA0lEQVR4nO2STS4EURSFz71P9WuiBqUTbEBILEAYkN6EoYFFMRGRmNkIdiAmYipNdadESnXddwyqX+nyk1iAM7r3fue8yTsYjkeTimRgq0CymozGQ/GrLkmzzaMdChpRri/v82JqT80uinNazJ1Co1FU1XnsPzbU+LAL71RbAwSDW9YkWfMmk9ldZ9Q9F9H4mjt0IB3eIyyh7CYFBgIAESDdJIDwbZiDP+gf/h3q7yfCNZ8hULALxeCjv4/wpQk2SCNczqxNiohqD9vr8dm1LfZURT4biDNaCCEEM57EakqSqU9XNo735kt9cfdSVJbLwVXt+kuJlYZAUlREF930rbSFww+BXntYJHbrBQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"I\" (99.999%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABNUlEQVR4nIWSz0oCURSHv3PmGoJhZUSKpmYLcdOmB+l9WgbteoeeoCdxEbVoGYSWELgIcpwY554W458ZTD2rC9895zv87pXYGWAYLA8YmO31XVgWQFgrr186wdYBgPGm4f8IjHeXg+ZFFoqATxdiK6EEgPeIgIbDbKfJqN+t7geaTv3+yMLEvVwXS41m87zTOj4q/wxduNpWEPx4/AxQqlWLiZtk9zO8Hp60LzrN7u1jQHas0ry5qterDrxWCJI87N0BeDPTmISsE8yDqIIhQD4ECXIhuWkuoUSEZUa4KPsi4phLnaWdS+j19eHyrFE5CPAU5nARrtfBvZvpabXWavcGGC4yWawrCKp+NHpK7+Kms0IuIY9K6gUXxUu4UK9Smc7YWDtgvAVGs03fb9fYaBv83eL8AxTpf79+kXXmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"F\" (100.000%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABl0lEQVR4nGWSTWtUMRSGn+TG0apYi84oCG4EXSjYRXFsuxJcWERRcO3v0D8g/gKXgrgXuhRFpIODU0RBsCKIZRZCC9UKRaVzk7wukjtzbz2bfLw575OTE0MVRjO9Y3RsXpabrc/UwnX23VaQJHk9abUPu5rof4SzxJy6Mtr2lkZcwQCo8ANk6orh6DDZBn3dj6GeaTl3WhYg8na3EE3by8Ts8Q7DHnExjwW9fGyCPPSlQg6naVwHywUpV7lMQcPWMk/I837eGYfoogzoN8vH4D5WyO/H/0Oe+auYkC+To62Jlw6E9HispMSJKOYy0rK6B4lhkJBRWyfHyMI5V2A59TMhg95kzRkTAKyNszOpl9H2sAHAiavX2hvLfUs3N9oyqFDuqWKMegCv5BPy14kK+VBlWY6C5g9uVlU+rzR3l8LAqHV+p5NcxTMKn1QfJQWv7v3kWurT1LiQDyqloMesKkih1M7i5GVu7KrU2k1uyXsfo9YX6p1aWo96xPV0nW/3pmuasfHInaWLU233Z3v4/sXr38ZMPs8/w8HqnaCVo7IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"J\" (99.999%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABoUlEQVR4nG2Sv2tUURCFv7n37qrRmCAiISAWQmzDmi0UQRQLQ9DGTtHewugfoGLhXxEsxCKNIJhUIkRJRI1gAooSWVOFGBCihUX2+d67x+K52R9vp7yH8905MwPtcoznipKkKOWatg7RdPxqfrh+BmSNFz93ZinXF6XaPIqD0PVuzmIlEJnf2JP52GtznEyV6yIecD2iUQup21gm9hHFJJG3v73KosXhGp7XffoEz3nlyk4Upl4s58hYWUd9xMgF4F0WVKY6xv4o0xSestOoH0j9r5WCWvpzErG85YrhBDAPMQJYPlDHWMK1JuesAAKeU4pREy1esDh45XTydNEEcJassrbGbq9jn5VHXceBY0GJnrRXdXBVSZroK+A4tq1U1/4HAXdrPFatFaR2KAvN923qRy1tKtdNPJ4ZJXpF+3T0ktEHjy9jGEMN/dXtXSroLpUiSuCGUv0Y6XB+m6NaDR4LDH9XU/c6jExrCmfejP3zampxL53X+lz3j2AMXlpVqg8jXRoDj9T8tPBmS1HbD/d1b8IcEzPruXYaz+6M4rq0fy2Wsie/fk7PAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"J\" (99.998%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACBklEQVR4nF2RTUjVQRTFfzPzDLLU1I0VSY8EwTYV7nIRQVSLILCvhYtwEblRQShXIVSQIURB0AcULV1EkRAV8RYRQYhCWCBFBQ8/yCiRVOz9Z+a0mLd4dlaXe+65l3sOAIa6NnJA88VW6tu7nj7fAlVHG0lkQ6GZHf2vStMXXs96eb1g39QIADjadLX/r6J+B0neB00uqTMJLWPyipmP8j5KUUEl9ZaFN+Xlo6TgvddiKcZQ0jMAyxVlipKilxR0fFhZFjW3DSwD8pKkGPSrMPrm6y6GFafXdATL7jVFSYrKLjVgqDKO/YWWgzOHyNGrLAnVhbXOgnFYR101GziXtnoNucMGMICzOIMNTMgJ5P5MfcnLgYAQCRjuns3/TDdXlvxOLJV4qeKqylhuwlRydpLtG1MpTFynw47zX6cSTfPpohT0nvVroVtl1us0roIwjlNPion0eoxzYFzOYFzOAbWfFZI/8/n+EzgDaYaGTuhL7kUVP6q4F2oO3O6hsaPn0cJ5aFkrhyJlWp18Nxt17f6ivAbAclkhCymxGCWFUiZluoUDw5nvKWYpxOCDFH2Q2nGAZVPfjKJiFiQpBElZSTfSz85SMyIvaW5ZIcp7r1jSYPndnGNUCw86ts5In8YVdO+t9G1z2Y0cJwfr4Y70Y0/1w4luaofGjlWYhaX1Q+F6HgfJC/4BLlt9EwpDa9kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"C\" (99.856%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACG0lEQVR4nG2RTUhUYRSGn/N930zjdaYUfyYFFSzaSAurRRuDsKCFQSjUNrcR/WwKCoQWQYtaZSAtAqHARRC1EKGIpCQLIkyLjGDIkaiFMs6M5r135p4W46BJ7/bh/TkcwPJIS7pdxZnTAIbp/8BIgz6DRDvTCNslYWzAIdrSjJSjf5CxCHmH0OJFxtptTnXBmAM6icz9d96m1/pdF8L47bdguaXl8iHMllDOqt5FwPJENb+bmNtQQhLDGl7CCALvVefj1cHiaHuhmSM4AaFpUXV8Czu6oOMtWAEMXVGo93CVNuFySW9a7Eb54e8fMoNYwMRIXl+e7sPaSpDlnAZ6EgvGcHBOJ9oRwBjAMaLr/j4MQtONfPFKrdnpeV6ssoBXqou7EOHEL41K+UhVVf1PvRinXGvUXAHFJIZyzsbLaoq+SGoVddSfT3ovX5uodaC4WoOGxFZ79gZh7cS0gY58QR/gAGOMMdbSnfULofZjHfXP1rxRIiACsaW2wc7JYu3U2AplOJX9/OP4xtFiaBjNfp1fftyKAWFPa6AzfoVp/EAqyqWWZsFEAA81+p3GAIbEiAa+ZntxBkCkw+NPpmKsqWtfcZrL56uPbZj6snAHi7Wxq5lvH+fe9GCrfxdJIKwBkAqpixVyRrUKjw35yaHn8eDMxfK6yvDTTQSS3h/FZn+KdjeWEsni5JpsMlxnOiwvNfdrIdgxNyPKFsZfMjgAc1jUoLAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"G\" (42.446%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACAklEQVR4nE2TTUtWURSFn33ufX0lfS1BDcFGRdCnNBAJGkiDJuEg6Cc0cBBNskk0CIqgiIL6A9EoCKIcBGmFERJJECUhEWY1sCzw+6PXe89eDe413KN9eNiLfc5ah9F1/S/X1Ph6/ceL630pIQG4rbxkUSMtHJ+XK/8wUCEAHdOKm7SbCmdVz9315jCBhMFyNNfLYAn7luSKmeYPQqBzVi5JmW6SGk0TipI29Izgyc+nRIDAZ5CtLiAgpedQAEZICvgdEahTVMuugDP21wQi/40wvITZagBmphBAvohwqgCI2YmAyL4U0FfAvNaOAZGHc0DCLWWSa6UVq9ArueeZ3rYSwFigVMIyBqm7hfTJyXkDUs5rQ3It1aDtntyVPe/HjBQormk03vXdvVWfHH71eo7EVcBmDGT547XOb6e3NR1YXhtfjEkESLhWLLRUI3BiRe6audFFYkDgTgm3kzYwoI0suhbOYAAMKZdcyzXMrPpeuWIedQkwGiYVJddyK0Zauh899gegpatQCFVAjGFAUDgXCBxpLmC6A0P8KZ4k0BMwjhEL+Z0ALJSbYMFyjha9swcDKpvHT8HY21eY7XRjQFvpEQ9IuVIGLOpjDUu5oCjFDQ1Xt0Yz6hQVY1RZHqPGOoDLW0L9dT/hqqJcvy42Eri/WASz+A5zj97J16aHBtoJgX/3fF2XdEcMmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"B\" (100.000%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB7UlEQVR4nHXRPUjVYRTH8e/znP//vvmWWZi9XDKJUAqvJg2VLSGKhENE2VCQQbQILTU0FDQ01NLSYIVKLdFQBA0NgqQZlJhZQ6AgNBQpGQ6pV+99ntNgF+7F22/9nBc4x5zd0lQy8sdiffuZt/3BjtTsdGr/2uCztDgAarsAuKd9gO1oJHlHJ1qxICKkOiWMyCsdlKjApSro8MvHEeucs1MxMs5FUOecNeP1Jvb6evxRhbeAqo8CFgDVubhm7cDv3e1qAUNoyEVZS6A6P6MtuXLyogJo2pT9w4KYDGhYrfMb0RBLIzTXmdGNKOxclAw3wjfDtgA9WbJB3bgLH3dOnXcE+RivEl/b8rnl2OXK23cXjc9DS/KkNdULdRUr3R9BCjo9Xx9Sn0rOLlUfSkwuOesLdoYSnXkxXDv6ftvz6ZuV3qzPO1WCMKQDBMCBE7B9RD/tKXIE+RIrD360jTU+iW5EZW5rNlztXT58rsj5WC3FyeQ7PV0MrYAxY2ZfMVyfPsfm/6KJk15HU/DQMAOqDf6bBbzIaj5u+oXR0jb71IIlueYk12s0Vv5TjL+WHH+AWMyVXUEYkSEdlKgIXSmgVyeSAGU9KQDuax/AwaOETf3fbyUwwcW9RyZWmhXrEzRcsNoaedlTU/Ph6gLW/wU9UquusNEm+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"B\" (99.999%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB6ElEQVR4nE2SPWgVURCFvzu7yVOCP0FMLJQgGBADIiLYBAsLLUQEbYMhqbTQQlAUG5Eg9hY2ooWFIDYKCoIEf5IUQQSfSEihYPJU8GkkaHgxu3eOxe6+5HRzz5yZM3MnABjedezEvh1dy1/r4y9nY1Bv/6+ZIAALNjqrCn8mx0avfdYbkkK37bncc5d7lZFrgoQU875ne3KzBNCdR7UDZ3qyQIHApmllkiTXWcwYaMgLJQm3tVKWekKaJjVGFEuSg5mKVlHHSSGQTmmlJO8pL7m5jQQgYUjLek0C/QulMNcLrDQyLk2QYIe7vfJWb3scvj/9FpEOouJJNCjSFBqnASfd265FE0IRBEMuT7fQRougyBqkW6lakpH3DfwrQ3V8f8/fap2ZTsHFciFSpivYj8oQdEKG3N3dYxJfYc1VsoNw94YsmJlZ+D2H1dcqbenqZWIx2NICNrk6SicYn9r2WtjTb6GSbgZRq8ioYM0HRR0CvYjVwQLBuPUz9SLcDaKzonYddRJGlLukqC8bQo3z7bNYugAJN5VFSVHDdPCw/F5legzBGJPnUa7muUNj5V3ETBoCQsLJebnnuUty95hHdy2OFM0Tui/NuCSPZc384/XtWOE8ca0bPLJ/Z896tZYX5xvvpj60QvD/depOz5lFcbQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"G\" (100.000%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB10lEQVR4nGWTT0tVURTFf3ufcwsEsyhwoEVRKE5SevZ4iGF/JCI/QEUfIBo0rc/SrGnTBk4iMqMS7JnaIH1BhU6K0shy4Lv37N3gvj9Ga3RgsdfZa511oES49dmbeRfJd2ZKRqg+N/8X5rt3BBDvezGaZyvLmbd0kKmBFOYBAlUrCr+GtikOrXvT70cAxsTjr3WilaQ2R4Y8s+UIGNM4i5vkLdnAJVJsvFPAD9ZwlorYuZIJchrfFZTxfjIWaHOajo+R8YwIapMHirj1CusMxoffvm6vAiI89j2fRyS00Vo7ItZfRXiKWGIf1D0Cp06a8hp6hkJ3Jfn0U0B54LlvnICbbtYNr0ZQ4ArG+42MyyQBwC3xZYEUxY4Oo9RxZj/uCUjquTdgvEWNwHVzt4udYJXhbc/9NkoUKpJnP+o4QQDCXuVIEa0OxEJqwJvfOKUT4wJOfRNHOTYBzBHaHowpnKXd4ChnD1vGctc7I4NEFhFQpkk0VjvBKpO9SXdekkCp4axtdbJxzpPYaOCgp8+gzHdTS73nEOZQII4OWmidATQfLEkBYpWUrTUovK06LkXc+YADcQal71G3k3kFZ6VR1uLuH/+vz0+GKV+Aq1ue9v2Dpm/eaDH8BQGUA8nCZSkCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"G\" (97.075%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABu0lEQVR4nG2SP2yNURjGn/PnflxUddCmXbSig0QiRURILE0MGEgMCAaJGCq0UYkYLBaJxCKIaEyGDmzCYmIQaUnqz4QICyqp29yIuN8552fo1Xu/3vtMJ+d5f2/e980jNWndqam7cmqV1danv+Bnn0w7c3Mk1jjbFnWaIERe2gZqm2wky/bdhb//VRv/EMm516av0w1ySMz1toxktb7C7DwpMNKCOl2FQxfJIy+WNjXq/ca0+qukxK4lqNMlOCbdJw/cLppGnZ+Yyaz2kBLf1xZG8joDI/IqvSYGTss3g+V3fOmScRonBJ6rCXU6CVfkZTVQIcLOxpWsK71ifoNKzmV6QMi5pcw5ayVZr8NwR06S035i5MfCSF4u9hw4L81s6kiSCX8ra0jd5x5mYYWV19BnIimyoJjqDz5ckHbMUkshbygCkUfDZelglUhaLAcSBJ5Jxno72nN5mZl8U071nfMTgxh1r/ptkpyG4WPH4tZe4wQSe+UkrycwquW+rkz9FVLOhJykLTWmVxbONUmIfF0tI13n7UBzppz2kRM4Lq++ucddhbwZZe+pTt3cJquj12SKWbQ6MjZoJekfoAb318udnl4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"A\" (100.000%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB20lEQVR4nGWRT0hUYRTFz73fGx2LymkoCgqpaJO4KBcVQRNIEKlBuWgpIa3C1i3aZGCCQauCokVEbYYIWoTawnETFVj0DwnRICOo1USkTjPvu6fFvDfvjZ7l9/vOvZxzBbGcD072du3ZkmV5Yeb2TyESKU69MsZa6oem2ZCRcw+vDPZ9sFrIsCuhis4qfw1uBHBgxRtr7IGLYYCbXD4GuBZ5wirp7XgCFVN2Dy2i6FgmSc8CXDLYyaQaFQMbOPcIPrIAAAQ/WDaKl/OU4VlYMyxJHhB2HpLP09saywAAXp6unIAoegM8ENcMKX8uFDLeox+V5/SNDHWZFnMdC9jfjTfz4BoI07sZ509nMWGp2hpUasQZoCSCtU6AanuPYv4tYesh1M611Xa8V25veo7C6mx8snp9qS/ODh/kdyO4tZ1xNw2I+5wInGTkOiv14hlTtV0DHA9Ji22quTixcrh9ZlotmqWZ0LS4SQUAXLjvIsaTLau13ZfAy2h1zgWQKb52CiDACKt8cW3xI1Z/dwMAckXyLByAAFfDf0byBhZZHu050jfylYxuFWCIJPmsFbfoo+ClbL1Vwc7Jb0vvxtqA/EvSGxneyaYy5/IOEGDz6KfK3y+PC6k+RAAEgv8pwuTJz1Zw0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"A\" (97.176%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB+0lEQVR4nG2SzYvOURiGr+c5Zz4a+Yr0lsSwsKUpZEgUKVuFUlgoFiwk/wM7UUN2ahYWFpqMRmKhyWKUEjWMqRFiMWqmGeb1/s45t8X7/mYh9+50Lc79XM9jADiFnScOb1m5ODc5MfIal+jEHHY/aKqTP2P7catZoP9hUkmtJOU/qahcX4bOpTmllIpUklRKqnQXM8DoHlJOJas8PTuwbd9dSaWlCzjgDKlVlPX2IAbOFWVlzfRhwDnloqT7K/AYPLo9VqWsy0RYM6GkpJt4ACBySVlJL/swjispaxj3ut6uRZWipR0EbillTa3H68FY+U1FlU4RfAcmbsx2lQ4UCwlh7KXEjSjMjEBAIJCpAMZWiOuA2R+hWnapDX0YxmawVpfs16M5txh7enp7e/u61zYAyGeHLQX+H9l9+96QKbefBcfM6uJL8VsDLFJKPWid6v3t+GKgBMCd1qfpqQ+z8/PNO4PFiz8/Snx21YA0OT7+6nPVrhsA8ZGIjanKeuMAHkMI1v1FRUnXcNjTUi66SE97+8bWnyqqdJKAcz6ppfntHbuBA00VVTpC8BLunam68qrVy+I39QBGE5wchgdHy5N3qAP7ySanCRGyTxxrLPymPtVuSsCq2rZ7+9jay965oKSWDhE7H9UIcE7/UtLXfv5R1qEDo0vTu3D+At4NEcrfYR6LAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"A\" (99.920%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABnElEQVR4nF2Sz0vUURTFP/d+v6thdMixkQl/zErJSZkRMYViEFeza9miZeA/0CraRvsW7UUQ2rXoLxArkVE0wZmomERhNBRDiijn+26LWfS9ns2Dd3jnnXvOBVCWn6I4KA+f986Jy1/TnhUKbasrCPW+TM2TyoMSdxVghrCDed05knVA2LbzPOJlP9hZEZThU9sURyq3Tux9pAilAvsWmXs4NkQzUaAMTa8KVfiIArOw6/0Is9BACdzmsuVI6TLORZtYQ2GUrxeRpubUv8VxWh1ioTTC5u/oypkducmBxSKW3Tp9dC8T0n++PX+XX+wIKPknHQv2H4kd3RkYQ4glDNdeL9S6qVEs3shVVzWAsGJ1+vvS6OeFPUaByWBviHwl811bR4mZEvbQtJ8oLETsI8RUoEFIZxCYhC1AmeDPF59PohVoAWSb1sq62IXCDzsaRNDiKN9+ur6Uco7DMzW0kmHPtyVMwy4CWoUdTxpT0ADQMleffZkJVbqfADixdu6an4HvdnwDATZs2eeDsmbPemu89PL6+ij3X/Xu/gGYII/AH58rxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"H\" (99.999%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB00lEQVR4nFWSMWtUURCFv5n7spIEQY0QJIUQg8YmCJaKjaCFdhaCYGmhjY2oKf0DQkCLlEJAMVGCjWyZwgiCrYKihEBQwWJxyQv73r33WLy3G53uzsc5czkzhue5F51skA69fOg5pDv3egGguAs4K4pSVjqDYxz7rSxFbQAEzimmVGkNBwKPNUip0nUA4+A3JUXdIgCBK8rK6h3HQdbfQjifECC+DExiZwcHjAFglACIvQRQxREUtBD2EogKb2AEIFMEB2oBJHD2KxFTpthvFGChEXJ+YaL3/Wsk7ENXxREMeAZUW2+Wf46kzuRiT1mSpFwnafdpTzlqE4eFz+puK0vaeNeTqlqS1MD5bd3gtWpJs8zc/KiU4giuK7+deq5a0hyO36+UR7DMUasrrbIIgWuxWcom7n2rmSqwJoaUO68ekIefXWKM5RK1b9W29CEMKZcfXWW1tXWg4LZiY1tYt/tPYEDmfQqNkSsU4T8otofrKUioDXus7UU114MD4gAGTLRwPIDRIQ8hiHEAjMNjAJ0CB9PkDIY4iQHG6U42Y3oaB2P+FE7mYmt7gYzlqbPN5KXmqP/M4hhHfyhJUeuAc+LXYLcsy76e4AQW1S/LsizLS38BQfYbYyF/vf0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"H\" (100.000%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACBUlEQVR4nE2STUhUURiGn++cOz9GYokKFWJkJUJqiDsVyoqEyIJoK7ho0SooSIjIRUW0qIWrcBWuKqKFFLnIjYJRBEaWZKVQFDkpOmIzjd6fr8WduXfO7pz3vLzP+50jQLo67WfXiVfljuTmWg6g49Gnpezy4sRAKAjnxr/+yWbmx3qBm+qpqq/v0wgIjKunqp4+BVJDOXX94HEDEjprH3i+G2yNVAGGtm/qX8CUEi19Of19HAHEYe/PeyQiURJcyraSEAASdDXGRjA0HsMBDBBw6D4ai8KNI2hRhOq+PrVRpt/ZvzsShSUG8QERQBlkIUQHhD0Z7SFlBTBJ2jcLh2MGQ7/ewkK6Ahwu6yAWwAEIzGiyu+1Ex4Htkl+Yedl07a745fC1w8N/NVBV3Xo4sj80hrkm6HyRaapsba6xK/MfVie7z4yZIPI1L+uruOUTLXRhi1WUoRpmcax1HMdaZkndwS9drVtTPRuygaVHNd9S7GJoyenniqg2vFGvN2KqW9GLxOPjvObb4ilMvbVlRuH5YrR3ONpS/mTCvtNFAnGomhgovS0gCU5N1+MIYNg1pRsnyzPbMzp3MDzo/6WuFm5XlT5Y+kpWXV2/mgKuq+sHgaevEwiI8EzdIPA9HcUw8y5rRDa/TPgAqkzO5UXMxsdpBEx9zbat9e//Itpkw85UYfWHy3/i8chBMhM7ZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"H\" (99.965%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB5klEQVR4nE2TT0iUURTFf/e+mamhcTDLRFAxK8vQ1i3SaNEi21SraN2qTa2C2kfbaF/RwkWrVhERRJsiIiHSbMwsCDWjyD9hzDjfvNPi+5zxwlscfu/ce+6DBwQeSFHpkaT60tuJiyUcA6Nn/OqQTLZY2ajS1j7QBVSuPXPSav8U6/FVRyo6z79TouRkqizPY1V1hUIIITiUJmNVH4opJDerRGMWUlngrBoNXQDA6VuXVvvIppi1Lase73vq3N/W4McCyiLobwXZSHb3CAmz0TOowC+gK4MjiOmtrmBsACUHEEdxKmyrOkbZAYulfvKqEFvQATlgdPfKVuebeRBFoJrCwVyDL2stn9gJrKZwmIS5GLY5y4ifDoghjBmsGVaFXuCbgzXsMM5UaySUu3GmHKB8iFwy14LGwC45rx2MA7tlS4st6HbcE1uYdHAGvc73dWvCqDGiXqw4wBBiZtvjqWOUYA9xiBwDplphXef2bebevHQwwkfVdZqwZQyF6VjTKQI4PSvSv4Fm2zw3VdVdAhBsVInmC2apLXBms6bneTewHdzQpr5CCCEE4NKa9LScRWifjY0Yb3UCUDzxSKrezmOA7Rm/PiwDlj//qfneg738fnLnPenS97JvkEiSajMTl/shpD3/A18P17tp0dBiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"D\" (100.000%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABqklEQVR4nI2SsWoUURSGv3vvmfGu7tikiEqQBY3NloKdZXrLVFbbhXQG4jNI3kDQB7BQsE9qIYUEm2QDgYhLCht3dTYz995jMTM6GhD/7pyP83POzwEAn21XZdWqrLYzD4AAsKTMMlpllHUNgAGDTG6MymRbmOzg7PvLgDalX+ghPR3qwre2zmQXd+ZiU4tsmi8vsqARvGydTDdHd6WTg7XR5vRkS7wsKe7z9azvymfW71GEYJ6FcWb3jp4+bBZSc/4Cl8Y7qf4kqO4DvNFOx+34vqpcukpcqr+F0BzOj6Eta5PHKkQRZ4IarKQn5zbh4uNj+/yVC9GIkd9rfJgBcOs2w7bTg0NrExK8Eq/ClEikpAbTxcE/1Ju01oK1RrvI+3CRElQsDe4qfNScsj6ziw4GVXGJFNLbpvPxgS2JudEQ5Rp5CHBTOo/rC0AvyUVkJ4xf272j97NfwZsm+Ok7AXZVN/4+YkN1F8SH+dSsjNKXrq+RNVk51bkI4KQ4LQ8k714hl4PytBAHApF61RehZ1r41TrwP68JTLSnyR8J+TioYxdadIPMLQF+ApLn070OznevAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"D\" (90.928%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACAklEQVR4nG2Rz0vUURTFP/e9N+P4i/xFgpEILsIoi6gMBUuIiiCDjBatWgUFBf4BQcugP6AWSa2D/oKSqFaWFEGLCknQirQwlRFHv/PeaeHM6KKzu5x77uGcawC4NDn0fbG4UdrM1zc21BcKdbmGvM0HAEutu2b80Rw7Ef3XACCKI+tpcrjso3/8oGzeFfru7OHN9ubuFSVFDVfGMaVzVSrYqKKifrc575wLob240hmqrE6RnJhechGQimtzC65q705iiFc4wKDQMVXVOXo3pJQ0iAcw+tNYzZJrKivpRyOGucYCzQcLVdLzRJkyPcMTuL+wd0dgs9xnRWW67etytK69w8xZzbIvk5I2uoHGpxonbJltdXsiRA9LR/pbDoz2xdek7bOeCWWqIn4rYDWlRX8Mh+zlYlPnYeXelnysCR29JaWk9S7gpnSDsH01cEVlRX0KPljbsvpxZkClviFE4kPZojaz+S9IqpKRAQyYAmNXx/SGU3NLzbJrVUkpDeAdPc8vmbXOPdoqGc95RSX9bNpKgOeyrhMqDxokkfhYdAK8wa00WenB8UJlZbpbSZDnrN6bAea8tS4qKepMqAshOOieTfdqUY8rKmqmMuUvzkqn8RCa9i+vXojRJ5XGVQ759p5D+8j+TJGAq/ofJvBAGFn/tfq3VCqVlTKSEsJS3UMT8A94DQhofc+THQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"E\" (99.990%)\n",
      "(?, 7, 7, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB0ElEQVR4nF2SvWtVQRDFf7N3XqKSKGogRALyfE1EQeyjSEwjCgFRI6jBoHmCWtiYRgQFQdRSBLu0VnYW/gWBEAMpbIygjR8QiMSYd/Nedncs7se7eqqZnT2zc86sfP8qVDBwdkVimS26ao2xxyTdzM6JSgnHq2EpWzluWAXCu4vW7RXTOg5AVFVrPTtf7qOgurBjKiua995vd9L3ExRUCe7LkS1MbOiRmCChsTaZhGKgYOdJQHpX8nfny4kckSYGSfuJ/7OVppvtjlkpJcbOYRwie9Yz5od6b04Vw+uLWfWIjddNQDo9+5+6zCWzYN/6EejaeLuRJ2bm7SoKJKqqqrVk8EqmxhnADBEIPgBYsvprJEo2kFn0x6t2AxMqgC4fiy7ozTsCYgfPeGcg8dLHz2Iw8sNCtNW9CMKBdrGB01mrUx0fvTVRUGb9RtpqtX63L1MDlBnb9rYgAiIDGzlzOhOn/vl972R03gVcvHYiJGB8Gn64aSAJby21ORKqRjSnsgPpXzK/PohUjdj1LHPYcehntHtoRWjChfEiOtmOy+qkAtd3N7+sTFsY41+MHs2qXucaD24tlt8DcDZ0vVirC28m16r+Sti9UMbSt2T/4fVf/afp9gTEJr8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 100,
       "width": 100
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"E\" (88.257%)\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "template = ('\\nPrediction is \"{}\" ({:.3f}%)')\n",
    "for k, v in predict_sample.items():\n",
    "    for i in v:\n",
    "        predictions = classifier.predict(input_fn=lambda:input_func([X_test[i]], [y_test[i]], 1, tf.estimator.ModeKeys.PREDICT))\n",
    "        for pred_dict in predictions:\n",
    "            pred_dict = pred_dict\n",
    "        pred_class = pred_dict['classes']\n",
    "        pred_certainty = float(pred_dict['probabilities'][pred_class] * 100) / float(sum(pred_dict['probabilities']))\n",
    "        \n",
    "        display(Image(filename=X_test[i], width=100, height=100))\n",
    "        print(template.format(class_dict[pred_class], pred_certainty, class_dict[y_test[i]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
