{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "This dataset was created by Yaroslav Bulatov by taking some publicly available fonts and extracting glyphs from them to make a dataset similar to MNIST. There are 10 classes, with letters A-J.\n",
    "\n",
    "## Content\n",
    "\n",
    "A set of training and test images of letters from A to J on various typefaces. The images size is 28x28 pixels.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "The dataset can be found on Tensorflow github page as well as on the blog from Yaroslav, here.\n",
    "\n",
    "## Inspiration\n",
    "\n",
    "This is a pretty good dataset to train classifiers! According to Yaroslav:\n",
    "\n",
    "Judging by the examples, one would expect this to be a harder task than MNIST. This seems to be the case -- logistic regression on top of stacked auto-encoder with fine-tuning gets about 89% accuracy whereas same approach gives got 98% on MNIST. Dataset consists of small hand-cleaned part, about 19k instances, and large uncleaned dataset, 500k instances. Two parts have approximately 0.5% and 6.5% label error rate. I got this by looking through glyphs and counting how often my guess of the letter didn't match it's unicode value in the font file.\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanleung/anaconda3/envs/py3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the path names to images and labeling into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small/\n",
      "0 0\n",
      "10000 9998\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>notMNIST_small/I/Qml0d2lzZS50dGY=.png</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>notMNIST_small/I/RW5nbGFuZCBCb2xkSXRhbGljLnR0Z...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>notMNIST_small/I/R3JlZWsgSXRhbGljLnR0Zg==.png</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>notMNIST_small/I/Rmx5d2hlZWxTcXVhcmUudHRm.png</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>notMNIST_small/I/SGFuZGljYXAub3Rm.png</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0              notMNIST_small/I/Qml0d2lzZS50dGY=.png  I\n",
       "1  notMNIST_small/I/RW5nbGFuZCBCb2xkSXRhbGljLnR0Z...  I\n",
       "2      notMNIST_small/I/R3JlZWsgSXRhbGljLnR0Zg==.png  I\n",
       "3      notMNIST_small/I/Rmx5d2hlZWxTcXVhcmUudHRm.png  I\n",
       "4              notMNIST_small/I/SGFuZGljYXAub3Rm.png  I"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parentDir = 'notMNIST_small/'\n",
    "print(parentDir)\n",
    "data = []\n",
    "total = 0\n",
    "good = 0\n",
    "for folder in os.listdir(parentDir):\n",
    "    if folder != '.DS_Store':\n",
    "        for file in os.listdir(parentDir + folder):\n",
    "            if total % 10000 == 0:\n",
    "                print(total, good)\n",
    "            total += 1\n",
    "            try:\n",
    "                img_path = parentDir + folder + '/' + file\n",
    "                img = Image.open(img_path)\n",
    "                data.append([img_path, folder])\n",
    "                good += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "dataset = pd.DataFrame(data)\n",
    "dataset.head()\n",
    "\n",
    "\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18724\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set batch size and epochs\n",
    "\n",
    "Don't want batch size to be too large or not too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "def input_func(features, labels, batch_size):\n",
    "    \n",
    "    def parser(image, label): \n",
    "        \n",
    "        img = tf.image.decode_png(tf.read_file(image))\n",
    "        img = tf.image.resize_images(img, tf.constant([1, 784]))\n",
    "        img = tf.reshape(img, [28, 28, 1])\n",
    "        img = tf.cast(img, tf.float32, \"cast\")\n",
    "#         image = tf.reshape(image, [28, 28, 1])\n",
    "#         label = tf.one_hot(indices = label, depth = 10)\n",
    "        return img, label\n",
    "    \n",
    "# #     features = tf.convert_to_tensor(data[[i for i in range(784)]])\n",
    "# #     labels = tf.convert_to_tensor(pd.factorize(data['label'])[0])\n",
    "\n",
    "#     return tf.estimator.inputs.numpy_input_fn(\n",
    "#         x = {'x' : features},\n",
    "#         y = labels,\n",
    "#         batch_size = batch_size,\n",
    "#         num_epochs = num_epochs,\n",
    "#         shuffle = True\n",
    "#     )\n",
    "    \n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "#     feature_dict = {feature : tf.convert_to_tensor(data[feature]) for feature in data if feature != 'label'}\n",
    "#     labels = tf.convert_to_tensor(pd.factorize(data['label'])[0])\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((feature_dict, labels))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "#     dataset = dataset.repeat(num_epochs)\n",
    "    return dataset\n",
    "#     iterator = dataset.make_one_shot_iterator()\n",
    "#     features, labels = iterator.get_next()\n",
    "#     return features, labels\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model architecture\n",
    "Uses a two layer, each layer consisting of a convolutional and pooling layer, architecture. (Same architecture as original MNIST CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(features, labels, mode, params):\n",
    "    #initialize input by reshaping and casting for network\n",
    "    #img = tf.image.decode_png(tf.read_file(features['x'][0]))\n",
    "    # img = np.array( img, dtype='uint8' ).flatten()\n",
    "    \n",
    "    # FIRST LAYER\n",
    "    # ---conv layer with 32 filters, 5x5 kernel, and relu activation\n",
    "    # ---pool layer with 2x2 pool window and stride of 2x2\n",
    "    conv1 = tf.layers.conv2d(inputs=features, filters=32, kernel_size=(5, 5), padding=\"same\", activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(2, 2), strides=(2, 2))\n",
    "    \n",
    "    # SECOND LAYER\n",
    "    # ---conv layer with 64 filters, 5x5 kernel, and relu activation\n",
    "    # ---pool layer with 2x2 pool window and stride of 2x2\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=(5, 5), padding=\"same\", activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2, 2), strides=(2, 2))\n",
    "    \n",
    "    # DENSE LAYER\n",
    "    # ---flatten output into vector\n",
    "    # ---dropout to prevent overfitting\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "    onehot_labels = tf.reshape(onehot_labels, [-1, 10])\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
    "#     loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    print(labels.shape)\n",
    "    print(predictions[\"classes\"].shape)\n",
    "    eval_metric_ops = {\"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating\n",
    "-> split data into train and test (2:1)\n",
    "-> instantiate model with my_mode as cnn\n",
    "-> convert dataset (np array) to dataframe to use pd.factorize to get integer labels, then convert back to np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/th/svpqqvhs62790bm9gczzcth40000gn/T/tmpu_543l74\n",
      "INFO:tensorflow:Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a2fc3e9b0>, '_save_summary_steps': 100, '_session_config': None, '_task_id': 0, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_global_id_in_cluster': 0, '_log_step_count_steps': 100, '_master': '', '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_save_checkpoints_steps': None, '_tf_random_seed': None, '_evaluation_master': '', '_is_chief': True, '_model_dir': '/var/folders/th/svpqqvhs62790bm9gczzcth40000gn/T/tmpu_543l74', '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_task_type': 'worker', '_train_distribute': None}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function my_model at 0x1a305746a8>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/th/svpqqvhs62790bm9gczzcth40000gn/T/tmpu_543l74/model.ckpt.\n",
      "INFO:tensorflow:loss = 52.976013, step = 1\n",
      "INFO:tensorflow:global_step/sec: 10.1915\n",
      "INFO:tensorflow:loss = 1.6180887, step = 101 (9.813 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.32191\n",
      "INFO:tensorflow:loss = 1.3526199, step = 201 (13.665 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1898\n",
      "INFO:tensorflow:loss = 1.3854624, step = 301 (9.807 sec)\n"
     ]
    }
   ],
   "source": [
    "# Fetch the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[0], pd.factorize(dataset[1])[0], test_size=0.33, random_state=42)\n",
    "\n",
    "# Build CNN.\n",
    "classifier = tf.estimator.Estimator(model_fn=my_model)\n",
    "\n",
    "# Train the Model.\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "# print(X_train, y_train)\n",
    "\n",
    "# train_input_func = tf.estimator.inputs.numpy_input_fn(x = {'x' : X_train}, \n",
    "#                                                       y = y_train, \n",
    "#                                                       batch_size = batch_size, \n",
    "#                                                       num_epochs = num_epochs, \n",
    "#                                                       shuffle = True\n",
    "#                                                      )\n",
    "classifier.train(input_fn=lambda:input_func(X_train, y_train, batch_size), steps = 22000)\n",
    "\n",
    "# Evaluate the model.\n",
    "# eval_input_func = tf.estimator.inputs.numpy_input_fn(x = {'x' : X_test}, \n",
    "#                                                       y = y_test, \n",
    "#                                                       batch_size = batch_size, \n",
    "#                                                       num_epochs = num_epochs, \n",
    "#                                                       shuffle = True\n",
    "#                                                      )\n",
    "eval_result = classifier.evaluate(input_fn=lambda:input_func(X_test, y_test, batch_size))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notMNIST_large/J/RmF0Ym95IFNsaW0gQkxUQyAyIEJSSy50dGY=.png'\n",
      " 'notMNIST_large/F/Q29vcGVyIEJsYWNrIEJULnR0Zg==.png'\n",
      " 'notMNIST_large/C/Q3VzaGluZy1IZWF2eS5vdGY=.png' ...\n",
      " 'notMNIST_large/F/VGhyb2hhbmRQZW4tUm9tYW4ub3Rm.png'\n",
      " 'notMNIST_large/G/UmFndGltZVN0ZC5vdGY=.png'\n",
      " 'notMNIST_large/H/SW50ZXJzdGF0ZU1vbm8tTGd0Lm90Zg==.png'] 174608\n",
      "[5 3 6 ... 3 1 4] 174608\n"
     ]
    }
   ],
   "source": [
    "print(X_test, len(X_test))\n",
    "print(y_test, len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notMNIST_large/C/SXZhbGVuY2lhLUJvbGQub3Rm.png'\n",
      " 'notMNIST_large/H/V2hpc3RsZSBTdG9wIEpMLnR0Zg==.png'\n",
      " 'notMNIST_large/E/VHJhbnNpdGlvbmFsNTExQlQtQm9sZEl0YWxpYy5vdGY=.png' ...\n",
      " 'notMNIST_large/C/T2xkQm9sZC1MaWdodC50dGY=.png'\n",
      " 'notMNIST_large/A/UXVhTmF1dGljYWxlX0luaXRpYWxzX05vMS50dGY=.png'\n",
      " 'notMNIST_large/A/Q292ZS50dGY=.png'] 354506\n",
      "[6 4 8 ... 6 2 2] 354506\n"
     ]
    }
   ],
   "source": [
    "print(X_train, len(X_train))\n",
    "print(y_train, len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
